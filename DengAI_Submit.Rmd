---
title: "DengAI_Submit"
author: "Oren Jalon"
date: "November 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# DATA PREPARATION
## LIBRARIES
```{r library, echo = TRUE}
library(tidyverse)
library(caret)
library(reshape2)
```
## IMPORT DATA
```{r inport, echo = TRUE, message = FALSE}

d_train <- read_csv("dengue_features_train.csv")
d_labels <- read_csv("dengue_labels_train.csv")
d_test <- read_csv("dengue_features_test.csv")

```
## MERGE TRAIN and LABELS
```{r add labels, echo=TRUE}
library(tidyverse)
df <- d_train %>%
  inner_join(d_labels, by = (c("city","year","weekofyear")))

```
## FILTER BY CITY
```{r sj & iq test, echo=TRUE}
library(tidyverse)

#splitting by city and dropping week of year
sj <- df %>%
  filter (city == "sj") 

iq <- df %>%
  filter (city == "iq")

```
## SPLIT SUBMISSION FILE BY CITY
```{r submission, echo=TRUE}
d_submit <- read_csv("submission_format.csv")
#splitting by city and dropping week of year
sj_submit <- d_submit[d_submit$city == "sj",]
iq_submit <- d_submit[d_submit$city == "iq",]

```

# ***SJ ANALYSIS***
# PREPROCESSING
## TRAINING AND TEST SETS(VALIDATION)
```{r preprocessing_sj, echo = TRUE}

# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(sj$total_cases, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData <- sj[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- sj[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 1:24]
y = trainData$total_cases

```
## MISSING VALUES
```{r missing, echo = TRUE}
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData[,5:25], method='knnImpute')
preProcess_missingdata_model

# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)


```
## NORMALIZE DATA
```{r standardize, echo = TRUE}
preProcess_range_model <- preProcess(trainData[,5:25], method=c("range"))
trainData <- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$total_cases <- y

apply(trainData[, 2:25], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})

rm(preProcess_range_model)
```
# PREPARE TEST DATASET
## FILTER BY CITY
```{r prepare test, echo=TRUE}

#splitting by city and dropping week of year
sj_test <- d_test[d_test$city == "sj",]


```
## PROCESS TEST SET
```{r process test, echo = TRUE}

# Step 1: Impute missing values 
preProcess_missingdata_model <- preProcess(sj_test, method='knnImpute')
preProcess_missingdata_model
testData3 <- predict(preProcess_missingdata_model, sj_test)  
anyNA(testData3)

# Step 3: Transform the features to range between 0 and 1
preProcess_range_model <- preProcess(testData3[,5:24], method=c("range"))
testData4 <- predict(preProcess_range_model, newdata = testData3)

```
# MULTIPLE METHODS:CORR-TOTAL_CASES
Reducing the number of features to only those that are most correlated to total_cases
```{r ensemble corr_tc, echo = TRUE}
library(caret)
library(caretEnsemble)
#tuneGrid for the various models
grid_svmLinear <- expand.grid(C = c(0.75, 0.9, 1, 1.1, 1.25))

# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3,
                             savePredictions="final",
                             index=createFolds(trainData$total_cases, 5))

set.seed(100)

models.corr <- caretList(total_cases ~ reanalysis_specific_humidity_g_per_kg +
station_max_temp_c +
reanalysis_dew_point_temp_k +
station_avg_temp_c +
reanalysis_max_air_temp_k, 
                    data=trainData[,5:25], 
                    trControl=trainControl,
                    tuneList=list(
                      svmLinear=caretModelSpec(method="svmLinear", tuneGrid=grid_svmLinear)))

```
# PREDICT FOR TEST SET
```{r predict test, echo= TRUE}
predict_svmLinear <- predict(models.corr$svmLinear, testData4)

```
# COMPLETE SUBMISSION FILE FOR SJ
```{r submit sj, echo= TRUE}
sj_sol <- data.frame(sj_submit[,-4], total_cases = round(predict_svmLinear))
```
# ***IQ ANALYSIS***
# PREPROCESSING
## TRAINING AND TEST SETS(VALIDATION)
```{r preprocessing_iq, echo = TRUE}

# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(iq$total_cases, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData <- iq[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- iq[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 1:24]
y = trainData$total_cases


```
## MISSING VALUES
```{r missing_iq, echo = TRUE}
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData[,5:25], method='knnImpute')
preProcess_missingdata_model

# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)


```
## NORMALIZE DATA
```{r standardize_iq, echo = TRUE}
preProcess_range_model <- preProcess(trainData[,5:25], method=c("range"))
trainData <- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$total_cases <- y

apply(trainData[, 2:25], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})

rm(preProcess_range_model)
```
# PREPARE TEST DATASET
## FILTER BY CITY
```{r prepare test_iq, echo=TRUE}


#splitting by city and dropping week of year
iq_test <- d_test[d_test$city == "iq",]


```
## PROCESS TEST SET
```{r process test_iq, echo = TRUE}

# Step 1: Impute missing values 
preProcess_missingdata_model <- preProcess(iq_test, method='knnImpute')
preProcess_missingdata_model
testData3 <- predict(preProcess_missingdata_model, iq_test)  
anyNA(testData3)

# Step 3: Transform the features to range between 0 and 1
preProcess_range_model <- preProcess(testData3[,5:24], method=c("range"))
testData4 <- predict(preProcess_range_model, newdata = testData3)

```
# MULTIPLE METHODS:CORR-TOTAL_CASES
Reducing the number of features to only those that are most correlated to total_cases
```{r ensemble corr_tc_iq, echo = TRUE}
library(caret)
library(caretEnsemble)
#tuneGrid for the various models
grid_svmLinear <- expand.grid(C = c(0.75, 0.9, 1, 1.1, 1.25))

# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3,
                             savePredictions="final",
                             index=createFolds(trainData$total_cases, 5))

set.seed(100)

models.corr <- caretList(total_cases ~ reanalysis_specific_humidity_g_per_kg +
station_max_temp_c +
reanalysis_dew_point_temp_k +
station_avg_temp_c +
reanalysis_max_air_temp_k, 
                    data=trainData[,5:25], 
                    trControl=trainControl,
                    tuneList=list(
                      svmLinear=caretModelSpec(method="svmLinear", tuneGrid=grid_svmLinear)))

```
# PREDICT FOR TEST SET
```{r predict test_iq, echo= TRUE}
predict_svmLinear <- predict(models.corr$svmLinear, testData4)

```
# COMPLETE SUBMISSION FILE FOR IQ
```{r submit sj_iq, echo= TRUE}
iq_sol <- data.frame(iq_submit[,-4], total_cases = round(predict_svmLinear))
```
# ***MERGE IQ & SJ DATAFRAMES***
```{r merge df, echo=TRUE}
solution <- bind_rows(sj_sol,iq_sol)

write.csv(solution, file = 'predicted_solution.csv', row.names = F)

```

